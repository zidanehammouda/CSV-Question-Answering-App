{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=\"/disk/data/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/zidane/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(path)\n",
    "# device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:47<00:00, 55.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:21<00:00, 47.18s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/disk/data/huggingface/modules/transformers_modules/microsoft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m deepseek_model \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/deepseek-coder-7b-instruct-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m : AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/deepseek-coder-7b-instruct-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      6\u001b[0m mistral_model \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m : AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m phi2_model \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m : AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/proj_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:523\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 523\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/proj_env/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1147\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m   1146\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1147\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[1;32m   1151\u001b[0m         config_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class()\n",
      "File \u001b[0;32m~/anaconda3/envs/proj_env/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:489\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     code_revision \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m final_module \u001b[38;5;241m=\u001b[39m \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/proj_env/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:319\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n\u001b[0;32m--> 319\u001b[0m \u001b[43mcreate_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_submodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m submodule_path \u001b[38;5;241m=\u001b[39m Path(HF_MODULES_CACHE) \u001b[38;5;241m/\u001b[39m full_submodule\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m submodule \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(pretrained_model_name_or_path):\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# We copy local files to avoid putting too many folders in sys.path. This copy is done when the file is new or\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# has changed since last copy.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/proj_env/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:71\u001b[0m, in \u001b[0;36mcreate_dynamic_module\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# If the parent module does not exist yet, recursively create it.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dynamic_module_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mcreate_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_module_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(dynamic_module_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m init_path \u001b[38;5;241m=\u001b[39m dynamic_module_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/proj_env/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:72\u001b[0m, in \u001b[0;36mcreate_dynamic_module\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dynamic_module_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     71\u001b[0m     create_dynamic_module(dynamic_module_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_module_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m init_path \u001b[38;5;241m=\u001b[39m dynamic_module_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m init_path\u001b[38;5;241m.\u001b[39mexists():\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/disk/data/huggingface/modules/transformers_modules/microsoft'"
     ]
    }
   ],
   "source": [
    "deepseek_model = {\n",
    "    \"tokenizer\": AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-7b-instruct-v1.5\"),\n",
    "    \"model\" : AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-7b-instruct-v1.5\")\n",
    "}\n",
    "\n",
    "mistral_model = {\n",
    "    \"tokenizer\": AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\"),\n",
    "    \"model\" : AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "}\n",
    "\n",
    "phi2_model = {\n",
    "    \"tokenizer\" : AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True),\n",
    "    \"model\" : AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/zidane/anaconda3/envs/proj_env/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/zidane/anaconda3/envs/proj_env/lib/python3.11/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 49, in main\n",
      "    service.run()\n",
      "  File \"/home/zidane/anaconda3/envs/proj_env/lib/python3.11/site-packages/huggingface_hub/commands/scan_cache.py\", line 60, in run\n",
      "    hf_cache_info = scan_cache_dir(self.cache_dir)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zidane/anaconda3/envs/proj_env/lib/python3.11/site-packages/huggingface_hub/utils/_cache_manager.py\", line 607, in scan_cache_dir\n",
      "    repos.add(_scan_cached_repo(repo_path))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zidane/anaconda3/envs/proj_env/lib/python3.11/site-packages/huggingface_hub/utils/_cache_manager.py\", line 643, in _scan_cached_repo\n",
      "    if not snapshots_path.exists() or not snapshots_path.is_dir():\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zidane/anaconda3/envs/proj_env/lib/python3.11/pathlib.py\", line 1235, in exists\n",
      "    self.stat()\n",
      "  File \"/home/zidane/anaconda3/envs/proj_env/lib/python3.11/pathlib.py\", line 1013, in stat\n",
      "    return os.stat(self, follow_symlinks=follow_symlinks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "PermissionError: [Errno 13] Permission denied: '/disk/data/huggingface/hub/models--v2ray--Mixtral-8x22B-v0.1/snapshots'\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived                                        0\n",
      "Pclass                                          3\n",
      "Name                       Mr. Owen Harris Braund\n",
      "Sex                                          male\n",
      "Age                                          22.0\n",
      "Siblings/Spouses Aboard                         1\n",
      "Parents/Children Aboard                         0\n",
      "Fare                                         7.25\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_response(text):\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if matches:\n",
    "        return {'response_error':False,'response':matches[0]}\n",
    "    else:\n",
    "        return {'response_error':True,'response':text}\n",
    "\n",
    "def generate_response(prompt,model_name):\n",
    "    try:\n",
    "        coder_model_prompt = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        encodeds = model_name[\"tokenizer\"].apply_chat_template(coder_model_prompt, return_tensors=\"pt\")\n",
    "\n",
    "        model_inputs = encodeds.to(device)\n",
    "        model_name['model'].to(device)\n",
    "\n",
    "        generated_ids = model_name['model'].generate(model_inputs, max_new_tokens=500, do_sample=False,temperature=0.1,repetition_penalty=1)\n",
    "        decoded = model_name[\"tokenizer\"].batch_decode(generated_ids)\n",
    "        return decoded[0].split('[/INST]')[-1].split('</s>')[0]\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"df is a dataframe that {description}. df has these columns: {columns}. Write python code that answers this question: Print {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_description = 'containing the list of people that were on the titanic ship and some details such as age, sex, name and whether they survived or not'\n",
    "titanic_columns = '''['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the fare for Mr. Owen Harris Braund?\n",
    "answer1 = df[df['Name'] == \"Mr. Owen Harris Braund\"]['Fare'].iloc[0]\n",
    "\n",
    "# 2. How many females are in the dataset?\n",
    "answer2 = df[df['Sex'] == \"female\"].shape[0]\n",
    "\n",
    "# 3. What is the average age of the passengers?\n",
    "answer3 = df['Age'].mean()\n",
    "\n",
    "# 4. How many passengers survived?\n",
    "answer4 = df[df['Survived'] == 1].shape[0]\n",
    "\n",
    "# 5. Who paid the highest fare?\n",
    "answer5 = df.loc[df['Fare'].idxmax(), 'Name']\n",
    "\n",
    "# 6. What is the total amount of fare paid?\n",
    "answer6 = df['Fare'].sum()\n",
    "\n",
    "# 7. Who is the passenger that has the highest number of siblings abroad?\n",
    "answer7 = df.loc[df['Siblings/Spouses Aboard'].idxmax(), 'Name']\n",
    "\n",
    "# 8. What is the age of Miss. Laina Heikkinen?\n",
    "answer8 = df[df['Name'] == \"Miss. Laina Heikkinen\"]['Age'].iloc[0]\n",
    "\n",
    "# 9. How many passengers are in the 1st class?\n",
    "answer9 = df[df['Pclass'] == 1].shape[0]\n",
    "\n",
    "# 10. What is the average fare paid by passengers?\n",
    "answer10 = df['Fare'].mean()\n",
    "\n",
    "# 11. How many passengers have 0 siblings/spouses aboard?\n",
    "answer11 = df[df['Siblings/Spouses Aboard'] == 0].shape[0]\n",
    "\n",
    "# 12. Who is the oldest passenger in the dataset?\n",
    "answer12 = df.loc[df['Age'].idxmax(), 'Name']\n",
    "\n",
    "# 13. How many passengers did not survive?\n",
    "answer13 = df[df['Survived'] == 0].shape[0]\n",
    "\n",
    "# 14. What is the lowest fare paid in the dataset?\n",
    "answer14 = df['Fare'].min()\n",
    "\n",
    "test_cases_titanic = [\n",
    "    {'question': 'What is the fare for Mr. Owen Harris Braund?', 'answer': answer1},\n",
    "    {'question': 'How many females are in the dataset?', 'answer': answer2},\n",
    "    {'question': 'What is the average age of the passengers?', 'answer': answer3},\n",
    "    {'question': 'How many passengers survived?', 'answer': answer4},\n",
    "    {'question': 'Who paid the highest fare?', 'answer': answer5},\n",
    "    {'question': 'What is the total amount of fare paid?', 'answer': answer6},\n",
    "    {'question': 'Who is the passanger that has the highest number of siblings abroad ?', 'answer': answer7},\n",
    "    {\"question\": \"What is the age of Miss. Laina Heikkinen?\", \"answer\": answer8},\n",
    "    {\"question\": \"How many passengers are in the 1st class?\", \"answer\": answer9},\n",
    "    {\"question\": \"What is the average fare paid by passengers?\", \"answer\": answer10},\n",
    "    {\"question\": \"How many passengers have 0 siblings/spouses aboard?\", \"answer\": answer11},\n",
    "    {\"question\": \"Who is the oldest passenger in the dataset?\", \"answer\": answer12},\n",
    "    {\"question\": \"How many passengers did not survive?\", \"answer\": answer13},\n",
    "    {\"question\": \"What is the lowest fare paid in the dataset?\", \"answer\": answer14}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the monthly income of the first female in the dataset?\n",
    "answer1 = df.loc[df['Gender'] == \"Female\", 'Monthly Income'].iloc[0]\n",
    "\n",
    "# 2. How many students are there in the dataset?\n",
    "answer2 = (df['Occupation'] == \"Student\").sum()\n",
    "\n",
    "# 3. What is the average age of participants?\n",
    "answer3 = df['Age'].mean()\n",
    "\n",
    "# 4. How many participants provided positive feedback?\n",
    "answer4 = (df['Feedback'] == \"Positive\").sum()\n",
    "\n",
    "# 5. What is the educational qualification of the male participant?\n",
    "answer5 = df.loc[df['Gender'] == \"Male\", 'Educational Qualifications'].iloc[0]\n",
    "\n",
    "# 6. How many participants are single?\n",
    "answer6 = (df['Marital Status'] == \"Single\").sum()\n",
    "\n",
    "# 7. What is the most common marital status in the dataset?\n",
    "answer7 = df['Marital Status'].mode()[0]\n",
    "\n",
    "# 8. Are there any participants with a family size greater than 3?\n",
    "answer8 = \"Yes\" if any(df['Family size'] > 3) else \"No\"\n",
    "\n",
    "# 9. What is the pin code for the location with the highest latitude?\n",
    "answer9 = df.loc[df['latitude'].idxmax(), 'Pin code']\n",
    "\n",
    "# 10. Did any participant with negative feedback have a monthly income below Rs.10000?\n",
    "answer10 = \"Yes\" if any((df['Feedback'] == \"Negative\") & (df['Monthly Income'] == \"Below Rs.10000\")) else \"No\"\n",
    "\n",
    "test_cases_food=[\n",
    "    {\"question\": \"What is the monthly income of the first female in the dataset?\", \"answer\": answer1},\n",
    "    {\"question\": \"How many students are there in the dataset?\", \"answer\": answer2},\n",
    "    {\"question\": \"What is the average age of participants?\", \"answer\": answer3},\n",
    "    {\"question\": \"How many participants provided positive feedback?\", \"answer\": answer4},\n",
    "    {\"question\": \"What is the educational qualification of the male participant?\", \"answer\": answer5},\n",
    "    {\"question\": \"How many participants are single?\", \"answer\": answer6},\n",
    "    {\"question\": \"What is the most common marital status in the dataset?\", \"answer\": answer7},\n",
    "    {\"question\": \"Are there any participants with a family size greater than 3?\", \"answer\": answer8},\n",
    "    {\"question\": \"What is the pin code for the location with the highest latitude?\", \"answer\": answer9},\n",
    "    {\"question\": \"Did any participant with negative feedback have a monthly income below Rs.10000?\", \"answer\": answer10}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "def get_print(code):\n",
    "    buffer = io.StringIO()\n",
    "    sys.stdout = buffer\n",
    "\n",
    "    exec(code)\n",
    "\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "    return buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_description = 'The dataset contains information collected from an online food ordering platform over a period of time.'\n",
    "food_columns = \"['Age', 'Gender', 'Marital Status', 'Occupation', 'Monthly Income', 'Educational Qualifications', 'Family size', 'latitude', 'longitude', 'Pin code', 'Output', 'Feedback', 'Unnamed: 12']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  \n",
    "from tqdm import tqdm\n",
    "for tc in tqdm(test_cases_food):\n",
    "    prompt = prompt_template.format(description=food_description,columns=food_columns,question=tc['question'])\n",
    "    full_response = generate_response(prompt,deepseek_model)\n",
    "    extracted_code = extract_response(full_response)\n",
    "    try:\n",
    "        execution = get_print(extracted_code['response'])\n",
    "    except:\n",
    "        execution = 'Execution error'\n",
    "    data.append({\n",
    "        'question': tc['question'],\n",
    "        'answer': tc['answer'],\n",
    "        'response_error': extracted_code.get('response_error', None),  # Use .get to avoid KeyError if 'error' is missing\n",
    "        'response': extracted_code.get('response', None),\n",
    "        'execution': execution\n",
    "    })\n",
    "\n",
    "# Once the loop is complete, convert the list of dictionaries into a DataFrame\n",
    "testdf = pd.DataFrame(data)\n",
    "testdf.to_excel('food_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Height(Inches)\"</th>\n",
       "      <th>\"Weight(Pounds)\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>65.78</td>\n",
       "      <td>112.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>71.52</td>\n",
       "      <td>136.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>69.40</td>\n",
       "      <td>153.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>68.22</td>\n",
       "      <td>142.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>67.79</td>\n",
       "      <td>144.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index   Height(Inches)\"   \"Weight(Pounds)\"\n",
       "0      1             65.78             112.99\n",
       "1      2             71.52             136.49\n",
       "2      3             69.40             153.03\n",
       "3      4             68.22             142.34\n",
       "4      5             67.79             144.30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index', ' Height(Inches)\"', ' \"Weight(Pounds)\"'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPEN_API_KEY\"))\n",
    "\n",
    "def describe_file(df,filename):\n",
    "    description_prompt = \"\"\"This is an example row of a given dataset titled {filename}.\\n {first_row}. Complete this sentence with a maximum of 50 words: df is a dataframe about\"\"\"\n",
    "    first_row = df.iloc[0].to_dict()\n",
    "    prompt = description_prompt.format(filename=filename,first_row=first_row)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    text_response = response.choices[0].message.content\n",
    "    return text_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = \"hw_200.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "example_row = dict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_prompt = \"\"\"This is an example row of a given dataset titled {filename}.\\n {example_row}. Write 5 simple questions about the dataset so I can solve it using code.\"\"\"\n",
    "prompt = description_prompt.format(filename=filename,example_row=example_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example row of a given dataset titled hw_200.csv.\\n {\\'Index\\': 1.0, \\' Height(Inches)\"\\': 65.78, \\' \"Weight(Pounds)\"\\': 112.99}. Write 5 simple questions about the dataset so I can solve it using code.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. How many unique heights exist in the dataset?\\n2. What is the average weight of the individuals in the dataset?\\n3. What is the maximum and minimum height recorded in the dataset?\\n4. What is the correlation between the height and weight variables?\\n5. Can we find an individual in the dataset who is 66 inches tall and weighs 120 pounds?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy import stats\n",
      "\n",
      "# Assuming df is your DataFrame\n",
      "\n",
      "# Remove outliers of age\n",
      "z_scores = stats.zscore(df['Age'])\n",
      "abs_z_scores = np.abs(z_scores)\n",
      "filtered_entries = (abs_z_scores < 3)\n",
      "df_filtered = df[filtered_entries]\n",
      "\n",
      "# Plot and save the distribution of age\n",
      "plt.figure(figsize=(10,6))\n",
      "sns.distplot(df_filtered['Age'], bins=30, kde=True, color='blue')\n",
      "plt.title('Distribution of Age')\n",
      "plt.xlabel('Age')\n",
      "plt.ylabel('Frequency')\n",
      "plt.savefig('age_distribution.png')\n",
      "plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom scipy import stats\\n\\n# Assuming df is your DataFrame\\n\\n# Remove outliers of age\\nz_scores = stats.zscore(df['Age'])\\nabs_z_scores = np.abs(z_scores)\\nfiltered_entries = (abs_z_scores < 3)\\ndf_filtered = df[filtered_entries]\\n\\n# Plot and save the distribution of age\\nplt.figure(figsize=(10,6))\\nsns.distplot(df_filtered['Age'], bins=30, kde=True, color='blue')\\nplt.title('Distribution of Age')\\nplt.xlabel('Age')\\nplt.ylabel('Frequency')\\nplt.savefig('age_distribution.png')\\nplt.show()\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Remove outliers of age\n",
    "z_scores = stats.zscore(df['Age'])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3)\n",
    "df_filtered = df[filtered_entries]\n",
    "\n",
    "# Plot and save the distribution of age\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.distplot(df_filtered['Age'], bins=30, kde=True, color='blue')\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('age_distribution.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
